{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from logging import getLogger\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.model.general_recommender import BPR\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed, init_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model_name,\n",
    "               dataset_name,\n",
    "               parameter_dict):\n",
    "\n",
    "    # configurations initialization\n",
    "    config = Config(model=model_name, dataset=dataset_name, config_dict=parameter_dict)\n",
    "\n",
    "    # init random seed\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "\n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "\n",
    "    # write config info into log\n",
    "    logger.info(config)\n",
    "\n",
    "    # dataset creating and filtering\n",
    "    dataset = create_dataset(config)\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # dataset splitting\n",
    "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    model = BPR(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    # model training\n",
    "    best_valid_score, best_valid_result = trainer.fit(train_data, valid_data)\n",
    "    print('\\n\\nTraining best results')\n",
    "    print('best_valid_score: ', best_valid_score)\n",
    "    print('best_valid_result: ', best_valid_result)\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(test_data)\n",
    "    \n",
    "    print('\\n\\nTest results')\n",
    "    print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "VALID_METRIC = 'Recall@'+str(K)\n",
    "MODEL = 'BPR'\n",
    "SEED = 2020\n",
    "USE_GPU = False\n",
    "SHUFFLE = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sudden_drift_dataset 4000x7 part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01 Jan 16:20    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = False\n",
      "seed = 2020\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\n",
      "checkpoint_dir = processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 300\n",
      "train_batch_size = 2048\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}\n",
      "repeatable = False\n",
      "metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']\n",
      "topk = [3]\n",
      "valid_metric = Recall@3\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'item_id']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = False\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 64\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "device = cpu\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\data\\dataset\\dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=0, inplace=True)\n",
      "01 Jan 16:20    INFO  sudden_drift_dataset_4000x7_0.71_pt1\n",
      "The number of users: 488\n",
      "Average actions of users: 2.0533880903490758\n",
      "The number of items: 8\n",
      "Average actions of items: 142.85714285714286\n",
      "The number of inters: 1000\n",
      "The sparsity of the dataset: 74.38524590163935%\n",
      "Remain Fields: ['user_id', 'item_id']\n",
      "01 Jan 16:20    INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "01 Jan 16:20    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]\n",
      "01 Jan 16:20    INFO  BPR(\n",
      "  (user_embedding): Embedding(488, 64)\n",
      "  (item_embedding): Embedding(8, 64)\n",
      "  (loss): BPRLoss()\n",
      ")\n",
      "Trainable parameters: 31744\n",
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\trainer\\trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
      "01 Jan 16:20    INFO  epoch 0 training [time: 0.02s, train loss: 0.6934]\n",
      "01 Jan 16:20    INFO  epoch 0 evaluating [time: 0.02s, valid_score: 0.543300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5433    mrr@3 : 0.3123    ndcg@3 : 0.3716    hit@3 : 0.5433    precision@3 : 0.1811\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 1 training [time: 0.01s, train loss: 0.6940]\n",
      "01 Jan 16:20    INFO  epoch 1 evaluating [time: 0.01s, valid_score: 0.535400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5354    mrr@3 : 0.3097    ndcg@3 : 0.3676    hit@3 : 0.5354    precision@3 : 0.1785\n",
      "01 Jan 16:20    INFO  epoch 2 training [time: 0.01s, train loss: 0.6898]\n",
      "01 Jan 16:20    INFO  epoch 2 evaluating [time: 0.01s, valid_score: 0.543300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5433    mrr@3 : 0.3123    ndcg@3 : 0.3716    hit@3 : 0.5433    precision@3 : 0.1811\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 3 training [time: 0.01s, train loss: 0.6834]\n",
      "01 Jan 16:20    INFO  epoch 3 evaluating [time: 0.01s, valid_score: 0.543300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5433    mrr@3 : 0.3123    ndcg@3 : 0.3716    hit@3 : 0.5433    precision@3 : 0.1811\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 4 training [time: 0.01s, train loss: 0.6796]\n",
      "01 Jan 16:20    INFO  epoch 4 evaluating [time: 0.01s, valid_score: 0.551200]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5512    mrr@3 : 0.315    ndcg@3 : 0.3755    hit@3 : 0.5512    precision@3 : 0.1837\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 5 training [time: 0.01s, train loss: 0.6770]\n",
      "01 Jan 16:20    INFO  epoch 5 evaluating [time: 0.01s, valid_score: 0.559100]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5591    mrr@3 : 0.3255    ndcg@3 : 0.3853    hit@3 : 0.5591    precision@3 : 0.1864\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 6 training [time: 0.01s, train loss: 0.6769]\n",
      "01 Jan 16:20    INFO  epoch 6 evaluating [time: 0.01s, valid_score: 0.566900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5669    mrr@3 : 0.3281    ndcg@3 : 0.3892    hit@3 : 0.5669    precision@3 : 0.189\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 7 training [time: 0.01s, train loss: 0.6709]\n",
      "01 Jan 16:20    INFO  epoch 7 evaluating [time: 0.01s, valid_score: 0.566900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5669    mrr@3 : 0.3281    ndcg@3 : 0.3892    hit@3 : 0.5669    precision@3 : 0.189\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 8 training [time: 0.03s, train loss: 0.6668]\n",
      "01 Jan 16:20    INFO  epoch 8 evaluating [time: 0.02s, valid_score: 0.566900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5669    mrr@3 : 0.3281    ndcg@3 : 0.3892    hit@3 : 0.5669    precision@3 : 0.189\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 9 training [time: 0.00s, train loss: 0.6648]\n",
      "01 Jan 16:20    INFO  epoch 9 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.332    ndcg@3 : 0.3942    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 10 training [time: 0.01s, train loss: 0.6627]\n",
      "01 Jan 16:20    INFO  epoch 10 evaluating [time: 0.01s, valid_score: 0.566900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5669    mrr@3 : 0.3307    ndcg@3 : 0.3913    hit@3 : 0.5669    precision@3 : 0.189\n",
      "01 Jan 16:20    INFO  epoch 11 training [time: 0.01s, train loss: 0.6584]\n",
      "01 Jan 16:20    INFO  epoch 11 evaluating [time: 0.01s, valid_score: 0.566900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5669    mrr@3 : 0.3307    ndcg@3 : 0.3913    hit@3 : 0.5669    precision@3 : 0.189\n",
      "01 Jan 16:20    INFO  epoch 12 training [time: 0.01s, train loss: 0.6566]\n",
      "01 Jan 16:20    INFO  epoch 12 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3333    ndcg@3 : 0.3952    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 13 training [time: 0.01s, train loss: 0.6538]\n",
      "01 Jan 16:20    INFO  epoch 13 evaluating [time: 0.02s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3333    ndcg@3 : 0.3952    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 14 training [time: 0.01s, train loss: 0.6495]\n",
      "01 Jan 16:20    INFO  epoch 14 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3346    ndcg@3 : 0.3962    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 15 training [time: 0.01s, train loss: 0.6461]\n",
      "01 Jan 16:20    INFO  epoch 15 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3346    ndcg@3 : 0.3962    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 16 training [time: 0.01s, train loss: 0.6442]\n",
      "01 Jan 16:20    INFO  epoch 16 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3307    ndcg@3 : 0.3931    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 17 training [time: 0.06s, train loss: 0.6408]\n",
      "01 Jan 16:20    INFO  epoch 17 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3346    ndcg@3 : 0.396    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 18 training [time: 0.01s, train loss: 0.6340]\n",
      "01 Jan 16:20    INFO  epoch 18 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3386    ndcg@3 : 0.401    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 19 training [time: 0.00s, train loss: 0.6324]\n",
      "01 Jan 16:20    INFO  epoch 19 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3386    ndcg@3 : 0.401    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 20 training [time: 0.00s, train loss: 0.6291]\n",
      "01 Jan 16:20    INFO  epoch 20 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3386    ndcg@3 : 0.401    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 21 training [time: 0.01s, train loss: 0.6239]\n",
      "01 Jan 16:20    INFO  epoch 21 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3386    ndcg@3 : 0.401    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 22 training [time: 0.01s, train loss: 0.6209]\n",
      "01 Jan 16:20    INFO  epoch 22 evaluating [time: 0.02s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3386    ndcg@3 : 0.3989    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  epoch 23 training [time: 0.01s, train loss: 0.6168]\n",
      "01 Jan 16:20    INFO  epoch 23 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3412    ndcg@3 : 0.4029    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 24 training [time: 0.05s, train loss: 0.6137]\n",
      "01 Jan 16:20    INFO  epoch 24 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3373    ndcg@3 : 0.3979    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  epoch 25 training [time: 0.03s, train loss: 0.6102]\n",
      "01 Jan 16:20    INFO  epoch 25 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.336    ndcg@3 : 0.3971    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  epoch 26 training [time: 0.01s, train loss: 0.6066]\n",
      "01 Jan 16:20    INFO  epoch 26 evaluating [time: 0.01s, valid_score: 0.566900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5669    mrr@3 : 0.332    ndcg@3 : 0.3921    hit@3 : 0.5669    precision@3 : 0.189\n",
      "01 Jan 16:20    INFO  epoch 27 training [time: 0.01s, train loss: 0.6025]\n",
      "01 Jan 16:20    INFO  epoch 27 evaluating [time: 0.01s, valid_score: 0.566900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5669    mrr@3 : 0.332    ndcg@3 : 0.3921    hit@3 : 0.5669    precision@3 : 0.189\n",
      "01 Jan 16:20    INFO  epoch 28 training [time: 0.01s, train loss: 0.5961]\n",
      "01 Jan 16:20    INFO  epoch 28 evaluating [time: 0.02s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3386    ndcg@3 : 0.3989    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  epoch 29 training [time: 0.01s, train loss: 0.5930]\n",
      "01 Jan 16:20    INFO  epoch 29 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3399    ndcg@3 : 0.4    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  epoch 30 training [time: 0.05s, train loss: 0.5925]\n",
      "01 Jan 16:20    INFO  epoch 30 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3412    ndcg@3 : 0.401    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  epoch 31 training [time: 0.01s, train loss: 0.5899]\n",
      "01 Jan 16:20    INFO  epoch 31 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3451    ndcg@3 : 0.4039    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  epoch 32 training [time: 0.01s, train loss: 0.5855]\n",
      "01 Jan 16:20    INFO  epoch 32 evaluating [time: 0.01s, valid_score: 0.590600]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5906    mrr@3 : 0.3478    ndcg@3 : 0.4099    hit@3 : 0.5906    precision@3 : 0.1969\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 33 training [time: 0.01s, train loss: 0.5782]\n",
      "01 Jan 16:20    INFO  epoch 33 evaluating [time: 0.01s, valid_score: 0.606300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.6063    mrr@3 : 0.353    ndcg@3 : 0.4178    hit@3 : 0.6063    precision@3 : 0.2021\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 34 training [time: 0.00s, train loss: 0.5771]\n",
      "01 Jan 16:20    INFO  epoch 34 evaluating [time: 0.01s, valid_score: 0.606300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.6063    mrr@3 : 0.3517    ndcg@3 : 0.4168    hit@3 : 0.6063    precision@3 : 0.2021\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n",
      "01 Jan 16:20    INFO  epoch 35 training [time: 0.01s, train loss: 0.5731]\n",
      "01 Jan 16:20    INFO  epoch 35 evaluating [time: 0.01s, valid_score: 0.598400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5984    mrr@3 : 0.3491    ndcg@3 : 0.4128    hit@3 : 0.5984    precision@3 : 0.1995\n",
      "01 Jan 16:20    INFO  epoch 36 training [time: 0.01s, train loss: 0.5668]\n",
      "01 Jan 16:20    INFO  epoch 36 evaluating [time: 0.01s, valid_score: 0.598400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5984    mrr@3 : 0.353    ndcg@3 : 0.4157    hit@3 : 0.5984    precision@3 : 0.1995\n",
      "01 Jan 16:20    INFO  epoch 37 training [time: 0.01s, train loss: 0.5669]\n",
      "01 Jan 16:20    INFO  epoch 37 evaluating [time: 0.01s, valid_score: 0.598400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5984    mrr@3 : 0.3517    ndcg@3 : 0.4147    hit@3 : 0.5984    precision@3 : 0.1995\n",
      "01 Jan 16:20    INFO  epoch 38 training [time: 0.01s, train loss: 0.5611]\n",
      "01 Jan 16:20    INFO  epoch 38 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3504    ndcg@3 : 0.4097    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  epoch 39 training [time: 0.01s, train loss: 0.5573]\n",
      "01 Jan 16:20    INFO  epoch 39 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3504    ndcg@3 : 0.4097    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  epoch 40 training [time: 0.02s, train loss: 0.5503]\n",
      "01 Jan 16:20    INFO  epoch 40 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3491    ndcg@3 : 0.4087    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  epoch 41 training [time: 0.06s, train loss: 0.5478]\n",
      "01 Jan 16:20    INFO  epoch 41 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3451    ndcg@3 : 0.4058    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  epoch 42 training [time: 0.01s, train loss: 0.5442]\n",
      "01 Jan 16:20    INFO  epoch 42 evaluating [time: 0.01s, valid_score: 0.574800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5748    mrr@3 : 0.3412    ndcg@3 : 0.4008    hit@3 : 0.5748    precision@3 : 0.1916\n",
      "01 Jan 16:20    INFO  epoch 43 training [time: 0.01s, train loss: 0.5384]\n",
      "01 Jan 16:20    INFO  epoch 43 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3451    ndcg@3 : 0.4058    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  epoch 44 training [time: 0.01s, train loss: 0.5311]\n",
      "01 Jan 16:20    INFO  epoch 44 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3451    ndcg@3 : 0.4058    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  epoch 45 training [time: 0.01s, train loss: 0.5329]\n",
      "01 Jan 16:20    INFO  epoch 45 evaluating [time: 0.01s, valid_score: 0.582700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5827    mrr@3 : 0.3465    ndcg@3 : 0.4068    hit@3 : 0.5827    precision@3 : 0.1942\n",
      "01 Jan 16:20    INFO  Finished training, best eval result in epoch 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training best results\n",
      "best_valid_score:  0.6063\n",
      "best_valid_result:  OrderedDict([('recall@3', 0.6063), ('mrr@3', 0.3517), ('ndcg@3', 0.4168), ('hit@3', 0.6063), ('precision@3', 0.2021)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\trainer\\trainer.py:583: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=self.device)\n",
      "01 Jan 16:20    INFO  Loading model structure and parameters from processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt1\\BPR-Jan-01-2025_16-20-32.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test results\n",
      "OrderedDict([('recall@3', 0.5432), ('mrr@3', 0.3278), ('ndcg@3', 0.3825), ('hit@3', 0.5432), ('precision@3', 0.1811)])\n"
     ]
    }
   ],
   "source": [
    "dataset_name='sudden_drift_dataset_4000x7_0.71_pt1'\n",
    "data_path = 'processed_datasets/artificial_data/'\n",
    "parameter_dict = {\n",
    "    'dataset': dataset_name+'.inter',\n",
    "    'data_path': data_path,\n",
    "    'load_col': {'inter': ['user_id', 'item_id']},\n",
    "    'use_gpu':USE_GPU,\n",
    "    'topk':K,\n",
    "    'valid_metric':VALID_METRIC,\n",
    "    'checkpoint_dir':data_path+dataset_name,\n",
    "    'seed':SEED,\n",
    "    'shuffle': SHUFFLE\n",
    "}\n",
    "\n",
    "\n",
    "train_test(MODEL, dataset_name, parameter_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sudden drift dataset 4000x7 0.71 pt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01 Jan 16:20    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = False\n",
      "seed = 2020\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt2\n",
      "checkpoint_dir = processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt2\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 300\n",
      "train_batch_size = 2048\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}\n",
      "repeatable = False\n",
      "metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']\n",
      "topk = [3]\n",
      "valid_metric = Recall@3\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'item_id']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = False\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 64\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "device = cpu\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\data\\dataset\\dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=0, inplace=True)\n",
      "01 Jan 16:20    INFO  sudden_drift_dataset_4000x7_0.71_pt2\n",
      "The number of users: 967\n",
      "Average actions of users: 2.070393374741201\n",
      "The number of items: 8\n",
      "Average actions of items: 285.7142857142857\n",
      "The number of inters: 2000\n",
      "The sparsity of the dataset: 74.14684591520165%\n",
      "Remain Fields: ['user_id', 'item_id']\n",
      "01 Jan 16:20    INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "01 Jan 16:20    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]\n",
      "01 Jan 16:20    INFO  BPR(\n",
      "  (user_embedding): Embedding(967, 64)\n",
      "  (item_embedding): Embedding(8, 64)\n",
      "  (loss): BPRLoss()\n",
      ")\n",
      "Trainable parameters: 62400\n",
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\trainer\\trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
      "01 Jan 16:20    INFO  epoch 0 training [time: 0.04s, train loss: 0.6939]\n",
      "01 Jan 16:20    INFO  epoch 0 evaluating [time: 0.02s, valid_score: 0.545500]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5455    mrr@3 : 0.3352    ndcg@3 : 0.3889    hit@3 : 0.5455    precision@3 : 0.1818\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt2\\BPR-Jan-01-2025_16-20-38.pth\n",
      "01 Jan 16:20    INFO  epoch 1 training [time: 0.01s, train loss: 0.6922]\n",
      "01 Jan 16:20    INFO  epoch 1 evaluating [time: 0.01s, valid_score: 0.553000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.553    mrr@3 : 0.3352    ndcg@3 : 0.3908    hit@3 : 0.553    precision@3 : 0.1843\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt2\\BPR-Jan-01-2025_16-20-38.pth\n",
      "01 Jan 16:20    INFO  epoch 2 training [time: 0.01s, train loss: 0.6863]\n",
      "01 Jan 16:20    INFO  epoch 2 evaluating [time: 0.01s, valid_score: 0.564400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5644    mrr@3 : 0.3365    ndcg@3 : 0.3947    hit@3 : 0.5644    precision@3 : 0.1881\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt2\\BPR-Jan-01-2025_16-20-38.pth\n",
      "01 Jan 16:20    INFO  epoch 3 training [time: 0.00s, train loss: 0.6818]\n",
      "01 Jan 16:20    INFO  epoch 3 evaluating [time: 0.05s, valid_score: 0.556800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5568    mrr@3 : 0.3314    ndcg@3 : 0.389    hit@3 : 0.5568    precision@3 : 0.1856\n",
      "01 Jan 16:20    INFO  epoch 4 training [time: 0.02s, train loss: 0.6777]\n",
      "01 Jan 16:20    INFO  epoch 4 evaluating [time: 0.02s, valid_score: 0.549200]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5492    mrr@3 : 0.3277    ndcg@3 : 0.3842    hit@3 : 0.5492    precision@3 : 0.1831\n",
      "01 Jan 16:20    INFO  epoch 5 training [time: 0.01s, train loss: 0.6744]\n",
      "01 Jan 16:20    INFO  epoch 5 evaluating [time: 0.02s, valid_score: 0.553000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.553    mrr@3 : 0.3295    ndcg@3 : 0.3865    hit@3 : 0.553    precision@3 : 0.1843\n",
      "01 Jan 16:20    INFO  epoch 6 training [time: 0.01s, train loss: 0.6718]\n",
      "01 Jan 16:20    INFO  epoch 6 evaluating [time: 0.02s, valid_score: 0.553000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.553    mrr@3 : 0.3321    ndcg@3 : 0.3883    hit@3 : 0.553    precision@3 : 0.1843\n",
      "01 Jan 16:20    INFO  epoch 7 training [time: 0.01s, train loss: 0.6672]\n",
      "01 Jan 16:20    INFO  epoch 7 evaluating [time: 0.01s, valid_score: 0.541700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5417    mrr@3 : 0.3251    ndcg@3 : 0.3803    hit@3 : 0.5417    precision@3 : 0.1806\n",
      "01 Jan 16:20    INFO  epoch 8 training [time: 0.01s, train loss: 0.6635]\n",
      "01 Jan 16:20    INFO  epoch 8 evaluating [time: 0.02s, valid_score: 0.530300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5303    mrr@3 : 0.3194    ndcg@3 : 0.3732    hit@3 : 0.5303    precision@3 : 0.1768\n",
      "01 Jan 16:20    INFO  epoch 9 training [time: 0.01s, train loss: 0.6602]\n",
      "01 Jan 16:20    INFO  epoch 9 evaluating [time: 0.01s, valid_score: 0.526500]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5265    mrr@3 : 0.3194    ndcg@3 : 0.3722    hit@3 : 0.5265    precision@3 : 0.1755\n",
      "01 Jan 16:20    INFO  epoch 10 training [time: 0.01s, train loss: 0.6555]\n",
      "01 Jan 16:20    INFO  epoch 10 evaluating [time: 0.02s, valid_score: 0.518900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5189    mrr@3 : 0.3182    ndcg@3 : 0.3694    hit@3 : 0.5189    precision@3 : 0.173\n",
      "01 Jan 16:20    INFO  epoch 11 training [time: 0.01s, train loss: 0.6533]\n",
      "01 Jan 16:20    INFO  epoch 11 evaluating [time: 0.02s, valid_score: 0.518900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5189    mrr@3 : 0.3182    ndcg@3 : 0.3694    hit@3 : 0.5189    precision@3 : 0.173\n",
      "01 Jan 16:20    INFO  epoch 12 training [time: 0.01s, train loss: 0.6492]\n",
      "01 Jan 16:20    INFO  epoch 12 evaluating [time: 0.05s, valid_score: 0.518900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5189    mrr@3 : 0.3207    ndcg@3 : 0.3712    hit@3 : 0.5189    precision@3 : 0.173\n",
      "01 Jan 16:20    INFO  epoch 13 training [time: 0.03s, train loss: 0.6450]\n",
      "01 Jan 16:20    INFO  epoch 13 evaluating [time: 0.03s, valid_score: 0.515200]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5152    mrr@3 : 0.3226    ndcg@3 : 0.3716    hit@3 : 0.5152    precision@3 : 0.1717\n",
      "01 Jan 16:20    INFO  Finished training, best eval result in epoch 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training best results\n",
      "best_valid_score:  0.5644\n",
      "best_valid_result:  OrderedDict([('recall@3', 0.5644), ('mrr@3', 0.3365), ('ndcg@3', 0.3947), ('hit@3', 0.5644), ('precision@3', 0.1881)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\trainer\\trainer.py:583: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=self.device)\n",
      "01 Jan 16:20    INFO  Loading model structure and parameters from processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt2\\BPR-Jan-01-2025_16-20-38.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test results\n",
      "OrderedDict([('recall@3', 0.5402), ('mrr@3', 0.3467), ('ndcg@3', 0.3962), ('hit@3', 0.5402), ('precision@3', 0.1801)])\n"
     ]
    }
   ],
   "source": [
    "dataset_name='sudden_drift_dataset_4000x7_0.71_pt2'\n",
    "data_path = 'processed_datasets/artificial_data/'\n",
    "parameter_dict = {\n",
    "    'dataset': dataset_name+'.inter',\n",
    "    'data_path': data_path,\n",
    "    'load_col': {'inter': ['user_id', 'item_id']},\n",
    "    'use_gpu':USE_GPU,\n",
    "    'topk':K,\n",
    "    'valid_metric':VALID_METRIC,\n",
    "    'checkpoint_dir':data_path+dataset_name,\n",
    "    'seed':SEED,\n",
    "    'shuffle': SHUFFLE\n",
    "}\n",
    "\n",
    "train_test(MODEL, dataset_name, parameter_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sudden drift dataset 4000x7 0.71 pt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01 Jan 16:20    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = False\n",
      "seed = 2020\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\n",
      "checkpoint_dir = processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 300\n",
      "train_batch_size = 2048\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}\n",
      "repeatable = False\n",
      "metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']\n",
      "topk = [3]\n",
      "valid_metric = Recall@3\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'item_id']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = False\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 64\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "device = cpu\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\data\\dataset\\dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=0, inplace=True)\n",
      "01 Jan 16:20    INFO  sudden_drift_dataset_4000x7_0.71_pt3\n",
      "The number of users: 1460\n",
      "Average actions of users: 2.0562028786840303\n",
      "The number of items: 8\n",
      "Average actions of items: 428.57142857142856\n",
      "The number of inters: 3000\n",
      "The sparsity of the dataset: 74.31506849315068%\n",
      "Remain Fields: ['user_id', 'item_id']\n",
      "01 Jan 16:20    INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "01 Jan 16:20    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]\n",
      "01 Jan 16:20    INFO  BPR(\n",
      "  (user_embedding): Embedding(1460, 64)\n",
      "  (item_embedding): Embedding(8, 64)\n",
      "  (loss): BPRLoss()\n",
      ")\n",
      "Trainable parameters: 93952\n",
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\trainer\\trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
      "01 Jan 16:20    INFO  epoch 0 training [time: 0.01s, train loss: 0.6941]\n",
      "01 Jan 16:20    INFO  epoch 0 evaluating [time: 0.01s, valid_score: 0.506400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5064    mrr@3 : 0.3209    ndcg@3 : 0.3682    hit@3 : 0.5064    precision@3 : 0.1688\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 1 training [time: 0.01s, train loss: 0.6896]\n",
      "01 Jan 16:20    INFO  epoch 1 evaluating [time: 0.02s, valid_score: 0.511600]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5116    mrr@3 : 0.3235    ndcg@3 : 0.3714    hit@3 : 0.5116    precision@3 : 0.1705\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 2 training [time: 0.01s, train loss: 0.6860]\n",
      "01 Jan 16:20    INFO  epoch 2 evaluating [time: 0.02s, valid_score: 0.519300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5193    mrr@3 : 0.3213    ndcg@3 : 0.3718    hit@3 : 0.5193    precision@3 : 0.1731\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 3 training [time: 0.01s, train loss: 0.6823]\n",
      "01 Jan 16:20    INFO  epoch 3 evaluating [time: 0.02s, valid_score: 0.527000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.527    mrr@3 : 0.3256    ndcg@3 : 0.377    hit@3 : 0.527    precision@3 : 0.1757\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 4 training [time: 0.01s, train loss: 0.6788]\n",
      "01 Jan 16:20    INFO  epoch 4 evaluating [time: 0.02s, valid_score: 0.519300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5193    mrr@3 : 0.3256    ndcg@3 : 0.3751    hit@3 : 0.5193    precision@3 : 0.1731\n",
      "01 Jan 16:20    INFO  epoch 5 training [time: 0.01s, train loss: 0.6764]\n",
      "01 Jan 16:20    INFO  epoch 5 evaluating [time: 0.02s, valid_score: 0.524400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5244    mrr@3 : 0.329    ndcg@3 : 0.379    hit@3 : 0.5244    precision@3 : 0.1748\n",
      "01 Jan 16:20    INFO  epoch 6 training [time: 0.01s, train loss: 0.6725]\n",
      "01 Jan 16:20    INFO  epoch 6 evaluating [time: 0.02s, valid_score: 0.534700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5347    mrr@3 : 0.3359    ndcg@3 : 0.3867    hit@3 : 0.5347    precision@3 : 0.1782\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 7 training [time: 0.02s, train loss: 0.6691]\n",
      "01 Jan 16:20    INFO  epoch 7 evaluating [time: 0.03s, valid_score: 0.539800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5398    mrr@3 : 0.3415    ndcg@3 : 0.3922    hit@3 : 0.5398    precision@3 : 0.1799\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 8 training [time: 0.00s, train loss: 0.6659]\n",
      "01 Jan 16:20    INFO  epoch 8 evaluating [time: 0.01s, valid_score: 0.542400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5424    mrr@3 : 0.3466    ndcg@3 : 0.3966    hit@3 : 0.5424    precision@3 : 0.1808\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 9 training [time: 0.01s, train loss: 0.6612]\n",
      "01 Jan 16:20    INFO  epoch 9 evaluating [time: 0.02s, valid_score: 0.542400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5424    mrr@3 : 0.3466    ndcg@3 : 0.3967    hit@3 : 0.5424    precision@3 : 0.1808\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 10 training [time: 0.01s, train loss: 0.6580]\n",
      "01 Jan 16:20    INFO  epoch 10 evaluating [time: 0.02s, valid_score: 0.542400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5424    mrr@3 : 0.3513    ndcg@3 : 0.4002    hit@3 : 0.5424    precision@3 : 0.1808\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 11 training [time: 0.01s, train loss: 0.6526]\n",
      "01 Jan 16:20    INFO  epoch 11 evaluating [time: 0.02s, valid_score: 0.539800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5398    mrr@3 : 0.35    ndcg@3 : 0.3986    hit@3 : 0.5398    precision@3 : 0.1799\n",
      "01 Jan 16:20    INFO  epoch 12 training [time: 0.01s, train loss: 0.6510]\n",
      "01 Jan 16:20    INFO  epoch 12 evaluating [time: 0.02s, valid_score: 0.542400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5424    mrr@3 : 0.3526    ndcg@3 : 0.4012    hit@3 : 0.5424    precision@3 : 0.1808\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 13 training [time: 0.01s, train loss: 0.6470]\n",
      "01 Jan 16:20    INFO  epoch 13 evaluating [time: 0.02s, valid_score: 0.539800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5398    mrr@3 : 0.3522    ndcg@3 : 0.4002    hit@3 : 0.5398    precision@3 : 0.1799\n",
      "01 Jan 16:20    INFO  epoch 14 training [time: 0.01s, train loss: 0.6419]\n",
      "01 Jan 16:20    INFO  epoch 14 evaluating [time: 0.05s, valid_score: 0.542400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5424    mrr@3 : 0.3543    ndcg@3 : 0.4025    hit@3 : 0.5424    precision@3 : 0.1808\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 15 training [time: 0.01s, train loss: 0.6379]\n",
      "01 Jan 16:20    INFO  epoch 15 evaluating [time: 0.02s, valid_score: 0.545000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.545    mrr@3 : 0.3552    ndcg@3 : 0.4037    hit@3 : 0.545    precision@3 : 0.1817\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 16 training [time: 0.01s, train loss: 0.6336]\n",
      "01 Jan 16:20    INFO  epoch 16 evaluating [time: 0.01s, valid_score: 0.547600]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5476    mrr@3 : 0.3539    ndcg@3 : 0.4034    hit@3 : 0.5476    precision@3 : 0.1825\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 17 training [time: 0.01s, train loss: 0.6304]\n",
      "01 Jan 16:20    INFO  epoch 17 evaluating [time: 0.02s, valid_score: 0.547600]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5476    mrr@3 : 0.3535    ndcg@3 : 0.4031    hit@3 : 0.5476    precision@3 : 0.1825\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 18 training [time: 0.01s, train loss: 0.6270]\n",
      "01 Jan 16:20    INFO  epoch 18 evaluating [time: 0.01s, valid_score: 0.550100]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5501    mrr@3 : 0.3539    ndcg@3 : 0.4041    hit@3 : 0.5501    precision@3 : 0.1834\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 19 training [time: 0.01s, train loss: 0.6225]\n",
      "01 Jan 16:20    INFO  epoch 19 evaluating [time: 0.02s, valid_score: 0.555300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5553    mrr@3 : 0.359    ndcg@3 : 0.4092    hit@3 : 0.5553    precision@3 : 0.1851\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 20 training [time: 0.01s, train loss: 0.6187]\n",
      "01 Jan 16:20    INFO  epoch 20 evaluating [time: 0.02s, valid_score: 0.563000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.563    mrr@3 : 0.3608    ndcg@3 : 0.4124    hit@3 : 0.563    precision@3 : 0.1877\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 21 training [time: 0.05s, train loss: 0.6141]\n",
      "01 Jan 16:20    INFO  epoch 21 evaluating [time: 0.02s, valid_score: 0.563000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.563    mrr@3 : 0.3595    ndcg@3 : 0.4115    hit@3 : 0.563    precision@3 : 0.1877\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 22 training [time: 0.01s, train loss: 0.6105]\n",
      "01 Jan 16:20    INFO  epoch 22 evaluating [time: 0.02s, valid_score: 0.568100]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5681    mrr@3 : 0.362    ndcg@3 : 0.4147    hit@3 : 0.5681    precision@3 : 0.1894\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 23 training [time: 0.01s, train loss: 0.6063]\n",
      "01 Jan 16:20    INFO  epoch 23 evaluating [time: 0.02s, valid_score: 0.570700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5707    mrr@3 : 0.3629    ndcg@3 : 0.416    hit@3 : 0.5707    precision@3 : 0.1902\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 24 training [time: 0.01s, train loss: 0.6026]\n",
      "01 Jan 16:20    INFO  epoch 24 evaluating [time: 0.02s, valid_score: 0.573300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5733    mrr@3 : 0.3625    ndcg@3 : 0.4164    hit@3 : 0.5733    precision@3 : 0.1911\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 25 training [time: 0.02s, train loss: 0.5982]\n",
      "01 Jan 16:20    INFO  epoch 25 evaluating [time: 0.02s, valid_score: 0.573300]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5733    mrr@3 : 0.3638    ndcg@3 : 0.4173    hit@3 : 0.5733    precision@3 : 0.1911\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 26 training [time: 0.01s, train loss: 0.5930]\n",
      "01 Jan 16:20    INFO  epoch 26 evaluating [time: 0.03s, valid_score: 0.578400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5784    mrr@3 : 0.3625    ndcg@3 : 0.4177    hit@3 : 0.5784    precision@3 : 0.1928\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 27 training [time: 0.05s, train loss: 0.5887]\n",
      "01 Jan 16:20    INFO  epoch 27 evaluating [time: 0.02s, valid_score: 0.588700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5887    mrr@3 : 0.3676    ndcg@3 : 0.4241    hit@3 : 0.5887    precision@3 : 0.1962\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 28 training [time: 0.01s, train loss: 0.5843]\n",
      "01 Jan 16:20    INFO  epoch 28 evaluating [time: 0.03s, valid_score: 0.588700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5887    mrr@3 : 0.3663    ndcg@3 : 0.4231    hit@3 : 0.5887    precision@3 : 0.1962\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n",
      "01 Jan 16:20    INFO  epoch 29 training [time: 0.01s, train loss: 0.5802]\n",
      "01 Jan 16:20    INFO  epoch 29 evaluating [time: 0.03s, valid_score: 0.581000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.581    mrr@3 : 0.3638    ndcg@3 : 0.4193    hit@3 : 0.581    precision@3 : 0.1937\n",
      "01 Jan 16:20    INFO  epoch 30 training [time: 0.01s, train loss: 0.5763]\n",
      "01 Jan 16:20    INFO  epoch 30 evaluating [time: 0.02s, valid_score: 0.578400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5784    mrr@3 : 0.3616    ndcg@3 : 0.4171    hit@3 : 0.5784    precision@3 : 0.1928\n",
      "01 Jan 16:20    INFO  epoch 31 training [time: 0.01s, train loss: 0.5723]\n",
      "01 Jan 16:20    INFO  epoch 31 evaluating [time: 0.02s, valid_score: 0.575800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5758    mrr@3 : 0.365    ndcg@3 : 0.419    hit@3 : 0.5758    precision@3 : 0.1919\n",
      "01 Jan 16:20    INFO  epoch 32 training [time: 0.01s, train loss: 0.5659]\n",
      "01 Jan 16:20    INFO  epoch 32 evaluating [time: 0.01s, valid_score: 0.578400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5784    mrr@3 : 0.3663    ndcg@3 : 0.4206    hit@3 : 0.5784    precision@3 : 0.1928\n",
      "01 Jan 16:20    INFO  epoch 33 training [time: 0.01s, train loss: 0.5622]\n",
      "01 Jan 16:20    INFO  epoch 33 evaluating [time: 0.02s, valid_score: 0.578400]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5784    mrr@3 : 0.3672    ndcg@3 : 0.4213    hit@3 : 0.5784    precision@3 : 0.1928\n",
      "01 Jan 16:20    INFO  epoch 34 training [time: 0.01s, train loss: 0.5570]\n",
      "01 Jan 16:20    INFO  epoch 34 evaluating [time: 0.02s, valid_score: 0.586100]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5861    mrr@3 : 0.3698    ndcg@3 : 0.4252    hit@3 : 0.5861    precision@3 : 0.1954\n",
      "01 Jan 16:20    INFO  epoch 35 training [time: 0.01s, train loss: 0.5532]\n",
      "01 Jan 16:20    INFO  epoch 35 evaluating [time: 0.02s, valid_score: 0.586100]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.5861    mrr@3 : 0.3698    ndcg@3 : 0.4252    hit@3 : 0.5861    precision@3 : 0.1954\n",
      "01 Jan 16:20    INFO  epoch 36 training [time: 0.01s, train loss: 0.5504]\n",
      "01 Jan 16:20    INFO  epoch 36 evaluating [time: 0.01s, valid_score: 0.581000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.581    mrr@3 : 0.3668    ndcg@3 : 0.4216    hit@3 : 0.581    precision@3 : 0.1937\n",
      "01 Jan 16:20    INFO  epoch 37 training [time: 0.01s, train loss: 0.5441]\n",
      "01 Jan 16:20    INFO  epoch 37 evaluating [time: 0.02s, valid_score: 0.581000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.581    mrr@3 : 0.3693    ndcg@3 : 0.4236    hit@3 : 0.581    precision@3 : 0.1937\n",
      "01 Jan 16:20    INFO  epoch 38 training [time: 0.01s, train loss: 0.5401]\n",
      "01 Jan 16:20    INFO  epoch 38 evaluating [time: 0.02s, valid_score: 0.581000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.581    mrr@3 : 0.3689    ndcg@3 : 0.4233    hit@3 : 0.581    precision@3 : 0.1937\n",
      "01 Jan 16:20    INFO  epoch 39 training [time: 0.01s, train loss: 0.5338]\n",
      "01 Jan 16:20    INFO  epoch 39 evaluating [time: 0.01s, valid_score: 0.581000]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.581    mrr@3 : 0.371    ndcg@3 : 0.4248    hit@3 : 0.581    precision@3 : 0.1937\n",
      "01 Jan 16:20    INFO  Finished training, best eval result in epoch 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training best results\n",
      "best_valid_score:  0.5887\n",
      "best_valid_result:  OrderedDict([('recall@3', 0.5887), ('mrr@3', 0.3663), ('ndcg@3', 0.4231), ('hit@3', 0.5887), ('precision@3', 0.1962)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\trainer\\trainer.py:583: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=self.device)\n",
      "01 Jan 16:20    INFO  Loading model structure and parameters from processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71_pt3\\BPR-Jan-01-2025_16-20-40.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test results\n",
      "OrderedDict([('recall@3', 0.5163), ('mrr@3', 0.3236), ('ndcg@3', 0.3727), ('hit@3', 0.5163), ('precision@3', 0.1721)])\n"
     ]
    }
   ],
   "source": [
    "dataset_name='sudden_drift_dataset_4000x7_0.71_pt3'\n",
    "data_path = 'processed_datasets/artificial_data/'\n",
    "parameter_dict = {\n",
    "    'dataset': dataset_name+'.inter',\n",
    "    'data_path': data_path,\n",
    "    'load_col': {'inter': ['user_id', 'item_id']},\n",
    "    'use_gpu':USE_GPU,\n",
    "    'topk':K,\n",
    "    'valid_metric':VALID_METRIC,\n",
    "    'checkpoint_dir':data_path+dataset_name,\n",
    "    'seed':SEED,\n",
    "    'shuffle': SHUFFLE\n",
    "}\n",
    "\n",
    "train_test(MODEL, dataset_name, parameter_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sudden drift dataset 4000x7 0.71 full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01 Jan 16:20    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = False\n",
      "seed = 2020\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71\n",
      "checkpoint_dir = processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 300\n",
      "train_batch_size = 2048\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}\n",
      "repeatable = False\n",
      "metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']\n",
      "topk = [3]\n",
      "valid_metric = Recall@3\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'item_id']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = False\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 64\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "device = cpu\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\data\\dataset\\dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=0, inplace=True)\n",
      "01 Jan 16:20    INFO  sudden_drift_dataset_4000x7_0.71\n",
      "The number of users: 4002\n",
      "Average actions of users: 2.0354911272181955\n",
      "The number of items: 8\n",
      "Average actions of items: 1163.4285714285713\n",
      "The number of inters: 8144\n",
      "The sparsity of the dataset: 74.56271864067966%\n",
      "Remain Fields: ['user_id', 'item_id']\n",
      "01 Jan 16:20    INFO  [Training]: train_batch_size = [2048] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "01 Jan 16:20    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]\n",
      "01 Jan 16:20    INFO  BPR(\n",
      "  (user_embedding): Embedding(4002, 64)\n",
      "  (item_embedding): Embedding(8, 64)\n",
      "  (loss): BPRLoss()\n",
      ")\n",
      "Trainable parameters: 256640\n",
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\trainer\\trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
      "01 Jan 16:20    INFO  epoch 0 training [time: 0.03s, train loss: 2.0792]\n",
      "01 Jan 16:20    INFO  epoch 0 evaluating [time: 0.06s, valid_score: 0.495600]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4956    mrr@3 : 0.3102    ndcg@3 : 0.3575    hit@3 : 0.4956    precision@3 : 0.1652\n",
      "01 Jan 16:20    INFO  Saving current: processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71\\BPR-Jan-01-2025_16-20-44.pth\n",
      "01 Jan 16:20    INFO  epoch 1 training [time: 0.03s, train loss: 2.0616]\n",
      "01 Jan 16:20    INFO  epoch 1 evaluating [time: 0.08s, valid_score: 0.494600]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4946    mrr@3 : 0.3107    ndcg@3 : 0.3576    hit@3 : 0.4946    precision@3 : 0.1649\n",
      "01 Jan 16:20    INFO  epoch 2 training [time: 0.03s, train loss: 2.0413]\n",
      "01 Jan 16:20    INFO  epoch 2 evaluating [time: 0.04s, valid_score: 0.485800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4858    mrr@3 : 0.305    ndcg@3 : 0.3511    hit@3 : 0.4858    precision@3 : 0.1619\n",
      "01 Jan 16:20    INFO  epoch 3 training [time: 0.03s, train loss: 2.0218]\n",
      "01 Jan 16:20    INFO  epoch 3 evaluating [time: 0.07s, valid_score: 0.483900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4839    mrr@3 : 0.299    ndcg@3 : 0.3461    hit@3 : 0.4839    precision@3 : 0.1613\n",
      "01 Jan 16:20    INFO  epoch 4 training [time: 0.03s, train loss: 2.0060]\n",
      "01 Jan 16:20    INFO  epoch 4 evaluating [time: 0.06s, valid_score: 0.484800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4848    mrr@3 : 0.2959    ndcg@3 : 0.3441    hit@3 : 0.4848    precision@3 : 0.1616\n",
      "01 Jan 16:20    INFO  epoch 5 training [time: 0.04s, train loss: 1.9839]\n",
      "01 Jan 16:20    INFO  epoch 5 evaluating [time: 0.06s, valid_score: 0.484800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4848    mrr@3 : 0.2946    ndcg@3 : 0.3431    hit@3 : 0.4848    precision@3 : 0.1616\n",
      "01 Jan 16:20    INFO  epoch 6 training [time: 0.03s, train loss: 1.9646]\n",
      "01 Jan 16:20    INFO  epoch 6 evaluating [time: 0.06s, valid_score: 0.482900]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4829    mrr@3 : 0.291    ndcg@3 : 0.3399    hit@3 : 0.4829    precision@3 : 0.161\n",
      "01 Jan 16:20    INFO  epoch 7 training [time: 0.03s, train loss: 1.9439]\n",
      "01 Jan 16:20    INFO  epoch 7 evaluating [time: 0.05s, valid_score: 0.486800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4868    mrr@3 : 0.2929    ndcg@3 : 0.3423    hit@3 : 0.4868    precision@3 : 0.1623\n",
      "01 Jan 16:20    INFO  epoch 8 training [time: 0.08s, train loss: 1.9215]\n",
      "01 Jan 16:20    INFO  epoch 8 evaluating [time: 0.06s, valid_score: 0.486800]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4868    mrr@3 : 0.2874    ndcg@3 : 0.3381    hit@3 : 0.4868    precision@3 : 0.1623\n",
      "01 Jan 16:20    INFO  epoch 9 training [time: 0.04s, train loss: 1.9007]\n",
      "01 Jan 16:20    INFO  epoch 9 evaluating [time: 0.06s, valid_score: 0.493600]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4936    mrr@3 : 0.2915    ndcg@3 : 0.3429    hit@3 : 0.4936    precision@3 : 0.1645\n",
      "01 Jan 16:20    INFO  epoch 10 training [time: 0.03s, train loss: 1.8756]\n",
      "01 Jan 16:20    INFO  epoch 10 evaluating [time: 0.06s, valid_score: 0.492700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4927    mrr@3 : 0.2908    ndcg@3 : 0.3422    hit@3 : 0.4927    precision@3 : 0.1642\n",
      "01 Jan 16:20    INFO  epoch 11 training [time: 0.02s, train loss: 1.8540]\n",
      "01 Jan 16:20    INFO  epoch 11 evaluating [time: 0.05s, valid_score: 0.490700]\n",
      "01 Jan 16:20    INFO  valid result: \n",
      "recall@3 : 0.4907    mrr@3 : 0.292    ndcg@3 : 0.3426    hit@3 : 0.4907    precision@3 : 0.1636\n",
      "01 Jan 16:20    INFO  Finished training, best eval result in epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training best results\n",
      "best_valid_score:  0.4956\n",
      "best_valid_result:  OrderedDict([('recall@3', 0.4956), ('mrr@3', 0.3102), ('ndcg@3', 0.3575), ('hit@3', 0.4956), ('precision@3', 0.1652)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mjlav\\anaconda3\\envs\\algorithms_transparency\\lib\\site-packages\\recbole\\trainer\\trainer.py:583: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file, map_location=self.device)\n",
      "01 Jan 16:20    INFO  Loading model structure and parameters from processed_datasets/artificial_data/sudden_drift_dataset_4000x7_0.71\\BPR-Jan-01-2025_16-20-44.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test results\n",
      "OrderedDict([('recall@3', 0.5655), ('mrr@3', 0.3436), ('ndcg@3', 0.4002), ('hit@3', 0.5655), ('precision@3', 0.1885)])\n"
     ]
    }
   ],
   "source": [
    "dataset_name='sudden_drift_dataset_4000x7_0.71'\n",
    "data_path = 'processed_datasets/artificial_data/'\n",
    "parameter_dict = {\n",
    "    'dataset': dataset_name+'.inter',\n",
    "    'data_path': data_path,\n",
    "    'load_col': {'inter': ['user_id', 'item_id']},\n",
    "    'use_gpu':USE_GPU,\n",
    "    'topk':K,\n",
    "    'valid_metric':VALID_METRIC,\n",
    "    'checkpoint_dir':data_path+dataset_name,\n",
    "    'seed':SEED,\n",
    "    'shuffle': SHUFFLE\n",
    "}\n",
    "\n",
    "train_test(MODEL, dataset_name, parameter_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorithms_transparency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
