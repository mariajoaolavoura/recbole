{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on feb 2nd 2025, I prompted DeepSeek with \n",
    "Considering the RecBole framework, using the BPR algorithm, how do I save the recommendations and test set in order to make the evaluation outside the framework?\n",
    "\n",
    "its response:\n",
    "\n",
    "\n",
    "2. Modify the Evaluation Code\n",
    "RecBole doesnâ€™t directly save recommendations by default, so weâ€™ll modify the evaluation process to save the recommendations and test set.\n",
    "\n",
    "Locate the evaluate method in the Trainer class:\n",
    "\n",
    "File Path: recbole/trainer/trainer.py\n",
    "\n",
    "Neighboring Functions: fit, _train_epoch, _valid_epoch\n",
    "\n",
    "Add a function to save recommendations and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def save_recommendations(test_data, predictions, file_path='recommendations.csv'):\n",
    "    \"\"\"Save recommendations to a CSV file.\"\"\"\n",
    "    user_ids = test_data['user_id'].numpy()\n",
    "    item_ids = test_data['item_id'].numpy()\n",
    "    scores = predictions.numpy()\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'user_id': user_ids,\n",
    "        'item_id': item_ids,\n",
    "        'score': scores\n",
    "    })\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def save_test_set(test_data, file_path='test_set.csv'):\n",
    "    \"\"\"Save the test set to a CSV file.\"\"\"\n",
    "    user_ids = test_data['user_id'].numpy()\n",
    "    item_ids = test_data['item_id'].numpy()\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'user_id': user_ids,\n",
    "        'item_id': item_ids\n",
    "    })\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Integrate Saving into the Evaluation\n",
    "Modify the evaluate method in the Trainer class to call these functions after evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, test_data, model, save_results=True):\n",
    "    # Original evaluation logic\n",
    "    predictions = model.predict(test_data)\n",
    "    \n",
    "    # Save recommendations and test set\n",
    "    if save_results:\n",
    "        save_recommendations(test_data, predictions, 'recommendations.csv')\n",
    "        save_test_set(test_data, 'test_set.csv')\n",
    "    \n",
    "    # Return evaluation metrics\n",
    "    return self._calculate_metrics(test_data, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the Experiment\n",
    "Use the run.py script to train and evaluate the BPR model. Ensure the modified Trainer class is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.quick_start import run_recbole\n",
    "from recbole.trainer import Trainer\n",
    "\n",
    "# Override the default Trainer with your modified version\n",
    "Trainer.evaluate = evaluate\n",
    "\n",
    "# Run the experiment\n",
    "run_recbole(model='BPR', dataset='your_dataset_name', config_file='bpr.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the save_recommendations, save_test_set and evaluate funtions are to be added to the recbole/trainer/trainer.py?\n",
    "No, you do not need to modify the original trainer.py file in the RecBole framework. Modifying the original source code is generally not recommended because it can lead to maintenance issues, especially when updating the framework. Instead, you can extend the Trainer class in your own script to add the save_recommendations, save_test_set, and evaluate functions.\n",
    "\n",
    "Hereâ€™s how you can do it in a clean and maintainable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from recbole.trainer import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def save_recommendations(self, test_data, predictions, file_path='recommendations.csv'):\n",
    "        \"\"\"Save recommendations to a CSV file.\"\"\"\n",
    "        user_ids = test_data['user_id'].numpy()\n",
    "        item_ids = test_data['item_id'].numpy()\n",
    "        scores = predictions.numpy()\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'user_id': user_ids,\n",
    "            'item_id': item_ids,\n",
    "            'score': scores\n",
    "        })\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "    def save_test_set(self, test_data, file_path='test_set.csv'):\n",
    "        \"\"\"Save the test set to a CSV file.\"\"\"\n",
    "        user_ids = test_data['user_id'].numpy()\n",
    "        item_ids = test_data['item_id'].numpy()\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'user_id': user_ids,\n",
    "            'item_id': item_ids\n",
    "        })\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "    def evaluate(self, test_data, model, save_results=True):\n",
    "        \"\"\"Override the evaluate method to save recommendations and test set.\"\"\"\n",
    "        # Original evaluation logic\n",
    "        predictions = model.predict(test_data)\n",
    "        \n",
    "        # Save recommendations and test set\n",
    "        if save_results:\n",
    "            self.save_recommendations(test_data, predictions, 'recommendations.csv')\n",
    "            self.save_test_set(test_data, 'test_set.csv')\n",
    "        \n",
    "        # Return evaluation metrics\n",
    "        return self._calculate_metrics(test_data, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.quick_start import run_recbole\n",
    "from custom_trainer import CustomTrainer\n",
    "\n",
    "# Override the default Trainer with your CustomTrainer\n",
    "run_recbole(\n",
    "    model='BPR',\n",
    "    dataset='your_dataset_name',\n",
    "    config_file='bpr.yaml',\n",
    "    trainer_class=CustomTrainer  # Pass your custom trainer class\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your_project/</br>\n",
    "â”œâ”€â”€ custom_trainer.py          # Your custom trainer class</br>\n",
    "â”œâ”€â”€ bpr.yaml                   # Configuration file</br>\n",
    "â”œâ”€â”€ run_experiment.py          # Script to run the experiment</br>\n",
    "â””â”€â”€ recommendations.csv        # Output file (generated after running)</br>\n",
    "â””â”€â”€ test_set.csv               # Output file (generated after running)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.model.general_recommender import BPR\n",
    "from custom_trainer import CustomTrainer\n",
    "\n",
    "# Load configuration\n",
    "config_dict = {\n",
    "    'dataset': 'your_dataset_name',\n",
    "    'model': 'BPR',\n",
    "    'eval_args': {\n",
    "        'split': {'RS': [0.8, 0.1, 0.1]},  # Train/Validation/Test split\n",
    "        'group_by': 'user',\n",
    "        'order': 'RO'\n",
    "    },\n",
    "    'metrics': ['Recall', 'Precision', 'NDCG']\n",
    "}\n",
    "config = Config(config_dict=config_dict)\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = create_dataset(config)\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "# Initialize model\n",
    "model = BPR(config, dataset).to(config['device'])\n",
    "\n",
    "# Initialize custom trainer\n",
    "trainer = CustomTrainer(config, model)\n",
    "\n",
    "# Train and evaluate\n",
    "best_valid_score, best_valid_result = trainer.fit(train_data, valid_data)\n",
    "test_result = trainer.evaluate(test_data, model)\n",
    "\n",
    "# Save recommendations and test set (already handled in the custom evaluate method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on 1st feb 2025, prompted chatgpt with\n",
    "\n",
    "Considering the RecBole framework, using the BPR algorithm, how do I save the recommendations and test set in order to make the evaluation outside the framework?\n",
    "\n",
    "the following code was its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from recbole.model.general_recommender.bpr import BPR\n",
    "from recbole.data.dataloader.general_dataloader import GeneralDataLoader\n",
    "\n",
    "# Load model configuration and dataset\n",
    "config = Config(model='BPR', dataset='your_dataset')  # Replace with actual dataset name\n",
    "dataset = create_dataset(config)\n",
    "test_data = dataset.build(split_ratio=[0.8, 0.1, 0.1])[2]  # Use the test set (last split)\n",
    "\n",
    "# Initialize the model\n",
    "model = BPR(config)\n",
    "\n",
    "# Load the trained model\n",
    "checkpoint = torch.load('path/to/saved/model.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Prepare DataLoader\n",
    "test_dataloader = GeneralDataLoader(config, dataset=test_data, batch_size=128)\n",
    "\n",
    "# Store recommendations\n",
    "recommendations = []\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        user_ids = batch['user_id']\n",
    "        item_ids = batch['item_id']\n",
    "        scores = model.predict(batch)\n",
    "\n",
    "        for u, i, s in zip(user_ids.tolist(), item_ids.tolist(), scores.tolist()):\n",
    "            recommendations.append((u, i, s))\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df_recommendations = pd.DataFrame(recommendations, columns=['user_id', 'item_id', 'score'])\n",
    "df_recommendations.to_csv(\"bpr_recommendations.csv\", index=False)\n",
    "\n",
    "print(\"Recommendations saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the test set interactions\n",
    "test_set = test_data.inter_feat[['user_id', 'item_id', 'rating']]  # Adjust column names as needed\n",
    "\n",
    "# Save to CSV\n",
    "test_set.to_csv(\"test_set.csv\", index=False)\n",
    "\n",
    "print(\"Test set saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how did you know it was the predict() method that was the key to solve what i asked?\n",
    "\n",
    "ChatGPT said:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(model_path, config)\n",
    "\n",
    "# Prepare test set\n",
    "interaction = current_test_data.inter_feat\n",
    "\n",
    "# Get predictions\n",
    "scores = model.predict(interaction)\n",
    "\n",
    "# Convert to DataFrame\n",
    "recommendations = pd.DataFrame({\n",
    "    'user': interaction[model.USER_ID].cpu().numpy(),\n",
    "    'item': interaction[model.ITEM_ID].cpu().numpy(),\n",
    "    'score': scores.cpu().detach().numpy()\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "recommendations.to_csv('recommendations.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recbole/</br> \n",
    "â”‚â”€â”€ model/</br> \n",
    "â”‚   â”œâ”€â”€ general_recommender/</br> \n",
    "â”‚   â”‚   â”œâ”€â”€ bpr.py</br> \n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ def forward(self, user, item)</br> \n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ def predict(self, interaction)  <-- ðŸ”¥ Key function!</br> \n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ def get_user_embedding(self, user)</br> \n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ def get_item_embedding(self, item)</br> \n",
    "â”‚â”€â”€ trainer/</br> \n",
    "â”‚   â”œâ”€â”€ trainer.py</br> \n",
    "â”‚   â”‚   â”œâ”€â”€ def evaluate(self, eval_data, load_best_model, model_file, show_progress)</br> \n",
    "â”‚   â”‚   â”œâ”€â”€ def _neg_sample_batch_eval(self, batched_data)</br> \n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ origin_scores = self.model.predict(interaction.to(self.device))  <-- ðŸ”¥ Calls `.predict()`</br> \n",
    "â”‚â”€â”€ utils/</br> \n",
    "â”‚   â”œâ”€â”€ case_study.py</br> \n",
    "â”‚   â”‚   â”œâ”€â”€ def load_model(model_file, config)  <-- ðŸ”¥ Loads model for evaluation</br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where are the recall, mr, ndcg, hit, precision calculated?\n",
    "\n",
    "ChatGPT said:\n",
    "\n",
    "The metrics like Recall, MRR (Mean Reciprocal Rank), NDCG (Normalized Discounted Cumulative Gain), \n",
    "Hit Rate, and Precision are calculated inside RecBoleâ€™s evaluation module, which is located in:\n",
    "\n",
    "recbole/\n",
    "\n",
    "â”‚â”€â”€ evaluator/</br> \n",
    "â”‚   â”œâ”€â”€ metrics.py    <-- ðŸ”¥ Defines the formulas for each metric</br> \n",
    "â”‚   â”œâ”€â”€ evaluator.py  <-- Calls the metric functions to evaluate models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorithms_transparency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
